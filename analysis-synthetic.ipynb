{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'myplots'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-68674ce0d922>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mompy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmyplots\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'myplots'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import logging\n",
    "import copy\n",
    "import dill as dill\n",
    "import pandas as pd\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from scipy.interpolate import interp1d\n",
    "from matplotlib import ticker\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# CHANGE THIS TO BE A SUBREPO\n",
    "import ompy as om\n",
    "\n",
    "import myplots as myplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc\n",
    "rc('font',**{'family':'serif','serif':['Computer Modern Roman']})\n",
    "rc('text', usetex=True)\n",
    "rc('axes', labelsize=\"large\")\n",
    "rc('errorbar', capsize=2.) # Set error bar style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "om.__full_version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducability we seed the random generator. \n",
    "# Note that by default several other classes in ompy, such as all\n",
    "# classes with multinest calculations have a default seed, too\n",
    "np.random.seed(1382398)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get smaller files for the online version\n",
    "plt.rcParams[\"figure.dpi\"] = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading raw data (particle - $\\gamma$-ray coincidence matrix)\n",
    "<a id='loading_data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $^{164}\\mathrm{Dy}$ data used below has been gathered from following experiment: Nyhus, H. T. *et al.* (2010). DOI: [10.1103/physrevc.81.024325](https://doi.org/10.1103/PhysRevC.81.024325)\n",
    "and is reanalyzed in RenstrÃ¸m, T. *et al.* (2018). DOI: [10.1103/physrevc.98.054310](https://doi.org/10.1103/PhysRevC.98.054310)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very similar notebooks are used for the *real* $^{164}\\mathrm{Dy}$, and for those where we have reduced the count statistics. Therefore we need to set some variables here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "case = widgets.RadioButtons(options = [\"164Dy: original\", \n",
    "                                       \"164Dy: reduced counts\",\n",
    "                                       \"164Dy: synthetic\"],\n",
    "                            description='Choose case:',\n",
    "                            index=0 # default selection\n",
    "                            )\n",
    "display(case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = \"164Dy: original\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Selected case: {case}\")\n",
    "if case.index == 0:\n",
    "    file_prefix = \"164Dy\"\n",
    "    counts_multiplyer = 1 # no change\n",
    "    Ex_max = 7700 # keV\n",
    "    nld_limit_low = [0.6, 1.7]\n",
    "    gsf_model_high_Efit = [4.5, 7]\n",
    "    fndiscrete = 'ompy/example_data/discrete_levels_Dy164.txt'\n",
    "    D0 = [6.8, 0.6]  # eV\n",
    "elif case.index == 1:\n",
    "    file_prefix = \"164Dy_reduced\"\n",
    "    counts_multiplyer = 1/20 # reducing statistics\n",
    "    Ex_max = 5900 # keV\n",
    "    nld_limit_low = [0.6, 1.7]\n",
    "    gsf_model_high_Efit = [4.5, 5.7]\n",
    "    fndiscrete = 'ompy/example_data/discrete_levels_Dy164.txt'\n",
    "    D0 = [6.8, 0.6]  # eV\n",
    "elif case.index == 2:\n",
    "    file_prefix = \"164Dy_synthetic\"\n",
    "    counts_multiplyer = 1 # reducing statistics\n",
    "    Ex_max = 7500 # keV\n",
    "    nld_limit_low = [0.4, 1.8]\n",
    "    gsf_model_high_Efit = [4.5, 7]\n",
    "    fndiscrete = 'RAINIER_164Dy/levels_Dy164_synthetic.txt'\n",
    "    ExEg_path = \"RAINIER_164Dy/ExEg.m\"\n",
    "    D0 = [7.4, 0.6]  # eV\n",
    "else:\n",
    "    raise NotImplementedError(f\"case: {case} is not implemented. Check spelling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the `regernerate` flag ensures, that we don't load from disk; which might result in expected results\n",
    "# if we have changed something in the input `raw` matrix.\n",
    "regenerate_response = False\n",
    "regenerate_ensemble = False\n",
    "regenerate_normalizers = False\n",
    "\n",
    "# number of ensemble members\n",
    "N_members = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import raw matrix into instance of om.Matrix() and plot it\n",
    "if case.index in [0,1]:\n",
    "    raw = om.example_raw('Dy164')\n",
    "    raw.plot(vmin=1)\n",
    "elif case.index == 2:\n",
    "    # there the \"raw\" matrix is still the unfolded matrix -- need to fold it below\n",
    "    raw = om.Matrix(path=ExEg_path)\n",
    "    raw.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut away unneccesary parts of the matrix \n",
    "# (-> shorter time to unfold & removed some remaining bg noise)\n",
    "raw.cut_diagonal(E1=(800, 0), E2=(7500, 7300))\n",
    "raw.cut('Ex', 0, 8400)\n",
    "raw.plot(vmin=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfolding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a response matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = om.introspection.get_logger('response', 'INFO')\n",
    "# Then do the same using OMpy functionality:\n",
    "# You may need to adpot this to whereever you response matrixes are stored\n",
    "folderpath = \"ompy/OCL_response_functions/nai2012_for_opt13\"\n",
    "\n",
    "# Energy calibration of resulting response matrix:\n",
    "Eg = raw.Eg\n",
    "\n",
    "# Experimental relative FWHM at 1.33 MeV of resulting array\n",
    "fwhm_abs = 90.44 # (90/1330 = 6.8%)\n",
    "\n",
    "# Magne recommends 1/10 of the actual resolution for unfolding purposes (see article)\n",
    "fname_resp = f'resp_{file_prefix}'\n",
    "\n",
    "fname_response = f'response_{file_prefix}'\n",
    "try:\n",
    "    if regenerate_response:\n",
    "        raise AssertionError(\"regenerate set to false; fall back to normalize\")\n",
    "    R_ompy_view = om.Matrix(path=fname_response+\"R_ompy_view.m\")\n",
    "    R_ompy_unf = om.Matrix(path=fname_response+\"R_ompy_unf.m\")\n",
    "    R_tab_view = pd.read_pickle(fname_response+\"R_tab_view.pkl\")\n",
    "    R_tab_unf = pd.read_pickle(fname_response+\"R_tab_unf.pkl\")\n",
    "except (OSError, FileNotFoundError, AssertionError) as e:\n",
    "    response = om.Response(folderpath)\n",
    "    R_ompy_view, R_tab_view = response.interpolate(Eg, fwhm_abs=fwhm_abs, return_table=True)\n",
    "    R_ompy_unf, R_tab_unf = response.interpolate(Eg, fwhm_abs=fwhm_abs/10, return_table=True)\n",
    "    \n",
    "    R_ompy_view.save(path=fname_response+\"R_ompy_view.m\")\n",
    "    R_ompy_unf.save(path=fname_response+\"R_ompy_unf.m\")\n",
    "    R_tab_view.to_pickle(fname_response+\"R_tab_view.pkl\")\n",
    "    R_tab_unf.to_pickle(fname_response+\"R_tab_unf.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the CACTUS setup, there is an additional experimenal threshold, that is not taken into account in the original response matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fthreshold = interp1d([30., 80., 122., 183., 244., 294., 344., 562., 779., 1000.],\n",
    "                      [0.0, 0.0, 0.0, 0.06, 0.44, 0.60, 0.87, 0.99, 1.00, 1.00],\n",
    "                      fill_value=\"extrapolate\")\n",
    "\n",
    "def apply_detector_threshold(response, table, fthreshold):\n",
    "    thres = fthreshold(response.Eg)\n",
    "    response.values = response.values * thres\n",
    "    # renormalize\n",
    "    response.values = om.div0(response.values, response.values.sum(axis=1)[:, np.newaxis])\n",
    "    table[\"eff_tot\"] *= thres\n",
    "\n",
    "apply_detector_threshold(R_ompy_unf, R_tab_unf, fthreshold)\n",
    "apply_detector_threshold(R_ompy_view, R_tab_view, fthreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the synthetic data, we need to fold the unfolded (all-gen) spectrum\n",
    "if case.index == 2:\n",
    "    # fold\n",
    "    raw = raw@R_ompy_view\n",
    "    # rebin Ex to have similar binning as exp sepctrum\n",
    "    raw.rebin(\"Ex\", factor=3)\n",
    "    raw.plot(vmin=1, vmax=1e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "R_ompy_view.plot(ax=ax, title=\"Response matrix\", vmin=5e-5, vmax=5e-1,\n",
    "                 scale=\"log\");\n",
    "ax.set_ylabel(r\"Incident $\\gamma$-ray energy $E_{\\gamma} [keV]$\")\n",
    "ax.set_xlabel(r\"Measured $\\gamma$-ray energy $E_{\\gamma} [keV]$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the unfolding & reduce statistics, if applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arificially reduce statistics\n",
    "raw_positive = raw.copy()\n",
    "raw_positive *= counts_multiplyer\n",
    "\n",
    "# round -- can't measure eg. 0.5 counts\n",
    "if case.index != 2:\n",
    "    raw_positive.values = np.around(raw_positive.values)\n",
    "\n",
    "# We need to remove negative counts (unphysical) in the raw matrix before unfolding:\n",
    "raw_positive.remove_negative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can decide to log information and set the logging level (info/debug)\n",
    "logger = om.introspection.get_logger('unfolder', 'INFO')\n",
    "\n",
    "# With compton subtraction and all tweaks\n",
    "unfolder= om.Unfolder(response=R_ompy_unf)\n",
    "unfolder.response_tab = R_tab_unf\n",
    "# Magne suggests some \"tweaks\" for a better unfolding performance. Default is 1 for all.\n",
    "unfolder.FWHM_tweak_multiplier = {\"fe\": 1., \"se\": 1.1,\n",
    "                                     \"de\": 1.3, \"511\": 0.9}\n",
    "unfolded = unfolder(raw_positive)\n",
    "unfolded.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.plot(scale=\"linear\", vmin=-1, vmax = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate the first generation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "firstgen = om.FirstGeneration()\n",
    "primary = firstgen(unfolded)\n",
    "primary.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagating statistical uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to propagate the statistical uncertainties from the raw matrix, we use an ensemble based method. We start of my generating en enseble of *raw-like* matrixes. The raw counts are poisson distributed. If we had counted one another time, we would get slightly different results. \n",
    "\n",
    "More precisely, the counts of the matrix containing `prompt+bg` events and the background events `bg` are each poisson distributed, where we have `raw = (prompt+bg) - bg_ratio * bg`. The ratio `bg_ratio` is determined by the ratio of the time gate lengths taken to obtain the `prompt+bg` and `bg` spectra. If a `bg` spectrum is provided to the `Ensemble` class, it will calculate the raw spectrum according to the equaltion above. Otherwise, the provided `raw` spectrum itself is assumed to be poisson distributed.\n",
    "\n",
    "For this experiment, we currently only have the `raw` matrix available, thus we propagate the error from there.\n",
    "\n",
    "We take the number of counts $k_i$ in bin $i$ of the raw matrix $R$ as an estimate for the Poisson parameter (\"the mean\") $Î»_i$ . Note that it is an unbiased estimator for $Î»_i$, since $E(k) = Î»$. To generate a member matrix $R_l$ of the MC ensemble, we replace the counts in each bin $i$ by a random draw from the distribution $\\operatorname{Poisson}(k_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class Ensemble() provides this feature. Its basic usage is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = om.introspection.get_logger('ensemble', 'INFO')\n",
    "\n",
    "# Tell the `Ensemble` class which raw spectrum, what kind of undolfer and first\n",
    "# generations method to use.\n",
    "# Note: This will have the same setting as above. We could for example have\n",
    "# set the first generations method to use a different \"valley_collection\", or a\n",
    "# differnt type of \"multiplicity_estimation\"\n",
    "ensemble = om.Ensemble(raw=raw_positive,  path=f\"ensemble_{file_prefix}\")\n",
    "ensemble.unfolder = unfolder\n",
    "ensemble.first_generation_method = firstgen\n",
    "# Generates N perturbated members;\n",
    "# the `regernerate` flag ensures, that we don't load from disk; which might result in expected results\n",
    "# if we have changed something in the input `raw` matrix.\n",
    "ensemble.generate(N_members, regenerate=regenerate_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated members are saved to disk and can be retrieved. Unfolded members can be retrieved as `ensemble.get_unfolded(i)`, for example. Their standard deviation is `ensemble.std_unfolded` for the unfolded matrixes, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the standard deviation of all ensemble members for the raw, unfolded and first generation spectrum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  # switch to inline due to problems with the backend on this machine\n",
    "#calling it a second time may prevent some graphics errors\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of the ensemble (mean)\n",
    "fig, ax = myplotsensemble_plot(ensemble)\n",
    "fig.set_size_inches((8, 3))\n",
    "\n",
    "def add_to_labels(ax, axis, pre=\"\", post=\" [keV]\"):\n",
    "    if axis == \"x\":\n",
    "        ax.set_xlabel(pre + ax.get_xlabel() + post)\n",
    "    elif axis == \"y\":\n",
    "        ax.set_ylabel(pre + ax.get_ylabel() + post)\n",
    "    else:\n",
    "        NotImplementedError(\"Only 'x' and 'y' are implemented.\")\n",
    "\n",
    "add_to_labels(ax[0], \"y\")\n",
    "for ax_ in ax:\n",
    "    locator = MaxNLocator(prune='both', nbins=5)\n",
    "    ax_.yaxis.set_major_locator(locator)\n",
    "    add_to_labels(ax_, \"x\")\n",
    "    \n",
    "\n",
    "fig.savefig(f\"figs/ensemble_{file_prefix}.jpeg\", dpi=300)\n",
    "\n",
    "# plot of the ensemble std. dev.\n",
    "fig, ax = ensemble.plot()\n",
    "ax[0].set_title(\"(d) $\\sigma_\\mathrm{raw}$\")\n",
    "ax[1].set_title(\"(e) $\\sigma_\\mathrm{unfolded}$\")\n",
    "ax[2].set_title(\"(f) $\\sigma_\\mathrm{first-generation}$\")\n",
    "fig.suptitle(\"\")\n",
    "\n",
    "add_to_labels(ax[0], \"y\")\n",
    "for ax_ in ax:\n",
    "    locator = MaxNLocator(prune='both', nbins=5)\n",
    "    ax_.yaxis.set_major_locator(locator)\n",
    "    add_to_labels(ax_, \"x\")\n",
    "\n",
    "fig.set_size_inches((8, 3))\n",
    "fig.savefig(f\"figs/ensemble2_{file_prefix}.jpeg\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook  # switch back for the analysis\n",
    "#calling it a second time may prevent some graphics errors\n",
    "%matplotlib notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Nuclear level density and gamma strength function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After matrix has been cut, unfolded and firstgen'd, perhaps ensembled, its nuclear level density (nld) and gamma strength function ($\\gamma$SF) can be extracted using the `Extractor()` class.  \n",
    "\n",
    "The method relies on the relation\n",
    " \\begin{align}\n",
    "\tP(E_x, E_\\gamma) \\propto NLD(E_x - E_\\gamma) \\mathcal{T}(E_\\gamma),\\label{eq:Oslo_method_eq}\n",
    "\\end{align}\n",
    "where $P(E_x, E_\\gamma)$ is the first-generation spectrum normalized to unity for each $E_x$ bin.  \n",
    "Furthermore, if we assume that the $\\gamma$ decay at high $E_x$ is dominated by dipole radiation the transmission coefficient $\\mathcal{T}$ is related to the dipole $\\gamma$-ray strength function $f(E_\\gamma)$ by the relation\n",
    "\\begin{align}\n",
    "    \\mathcal{T}(E_\\gamma) = 2\\pi E_\\gamma^3 f(E_\\gamma).\\label{eq:gammaSF}\n",
    "\\end{align} \n",
    "\n",
    "If you have reasons to assume a different multipose decomposition, you may of course calculate the transmission coefficient $\\mathcal{T}$ from the $\\gamma$-ray strength function produced here and apply the decomposition you prefer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When extracting NLD and GSF from an ensemble, a trapezoidal cutout must be performed on each ensemble member. This is achieved by `Action()` which allows for delayed function calls on matrices and vectors. This way we don't cut the raw matrix at `Ex_min`, but this will only happen before the extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trapezoid_cut = om.Action('matrix')\n",
    "trapezoid_cut.trapezoid(Ex_min=4400, Ex_max=Ex_max, Eg_min=1300, \n",
    "                        Eg_max=Ex_max+200, inplace=True)\n",
    "extractor = om.Extractor(path=f\"extraction_ensemble_{file_prefix}\")\n",
    "extractor.trapezoid = trapezoid_cut\n",
    "# Running the lines below directy, would most probably \n",
    "# result in a error like\n",
    "# The AssertionError: Ex and Eg must have the same step size\n",
    "#\n",
    "# Why? The extraction assumes that Ex and Eg have the same binning. Thus we\n",
    "# need to rebin the ensemble. This works will work inplace. \n",
    "# Note: As always, be careful will mid-bin vs lower bin calibration.\n",
    "# E_rebinned = ensemble.get_firstgen(0).Ex\n",
    "# \n",
    "E_rebinned = np.arange(100., 8500, 200)\n",
    "ensemble.rebin(E_rebinned, member=\"firstgen\")\n",
    "ensemble.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract the NLD and $\\gamma SF$ for $N$ of the samples of the ensemble.\n",
    "\n",
    "Note: \n",
    "The old software extended the decomposition beyond the `Ex=Eg` line by a resolution `dE`. This is now optional and we changed the default to not do this any longer, but rather assume that the rebinning above has been performed with a binsize of approx. the FWHM of the bin with the worst resolution (usually `(Ex_max, Eg_max)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extractor.extract_from(ensemble, regenerate=regenerate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting `nld` and `gsf` are saved to disk and exposed as `extractor.nld` and `extractor.gsf`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create another comparison Figure:**  \n",
    "    Normalized 1Gen spectrum from data, vs. the fitted product of NLD and $\\gamma$SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = ensemble.get_firstgen(0).copy()\n",
    "std = ensemble.std_firstgen.copy()\n",
    "trapezoid_cut.act_on(mat)\n",
    "trapezoid_cut.act_on(std)\n",
    "_, _, product = extractor.decompose(mat, std, product=True)\n",
    "\n",
    "fig, ax = plt.subplots(2,1, constrained_layout=True)\n",
    "fig.set_size_inches(4, 6)\n",
    "om.normalize_rows(mat.values) \n",
    "mat.plot(ax=ax[0], scale=\"log\", vmin=1e-3, vmax=1e-1, title=\"\")\n",
    "product.plot(ax=ax[1], scale=\"log\", vmin=1e-3, vmax=1e-1, title=\"\")\n",
    "\n",
    "x = np.linspace(*ax[0].get_ylim())\n",
    "ax[0].plot(x, x, \"r--\", label=\"E_x = E_g\")\n",
    "ax[1].plot(x, x, \"r--\", label=\"E_x = E_g\")\n",
    "\n",
    "ax[0].set_xlabel(r\"$\\gamma$-ray energy $E_\\gamma$ [keV]\")\n",
    "ax[0].set_ylabel(r\"Excitation energy $E_x$ [keV]\")\n",
    "ax[1].set_xlabel(r\"$\\gamma$-ray energy $E_\\gamma$ [keV]\")\n",
    "ax[1].set_ylabel(r\"Excitation energy $E_x$ [keV]\")\n",
    "\n",
    "ax[0].text(0.05, 0.05, r\"(a)\",\n",
    "           fontsize=plt.rcParams[\"axes.labelsize\"], \n",
    "           transform = ax[0].transAxes)\n",
    "\n",
    "ax[1].text(0.05, 0.05, r\"(b)\",\n",
    "           fontsize=plt.rcParams[\"axes.labelsize\"], \n",
    "           transform = ax[1].transAxes)\n",
    "\n",
    "fig.savefig(f\"figs/fg_vs_product_{file_prefix}.pdf\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results before normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extractor.plot(plot_mean=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or maybe you are more used to displaying the results with std. deviations?\n",
    "\n",
    "**Note**: This may be erroneous, as the nld and gsf are not normalized yet!  \n",
    "Thus, in principal, we might evaluate std. devs. of the *same solution* with different  \n",
    "transformations. Before we normalize, we don't know. And they have the same $\\chi^2$.  \n",
    "That was the reason for the *trouble* with normalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extractor.plot(plot_mean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's remove the nan-valued elements (unconstrained elements) for the further analysis\n",
    "extractor_org = copy.deepcopy(extractor)\n",
    "for nld in extractor.nld:\n",
    "    nld.cut_nan()\n",
    "for gsf in extractor.gsf:\n",
    "    gsf.cut_nan()\n",
    "\n",
    "# the \"mean\" nld at this stage; we'll use it later, but it's not a good estimate at this\n",
    "# stage (see article)\n",
    "nld_mean = extractor.ensemble_nld()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot for the article (before normalization):  \n",
    "Comparing the initial guess to the fitted solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, constrained_layout=True)\n",
    "fig.set_size_inches(4, 4.5)\n",
    "nld, gsf = extractor.nld[0].copy(), extractor.gsf[0].copy()\n",
    "\n",
    "nld.to_MeV()\n",
    "nld.plot(ax=ax[0], scale=\"log\", label=\"result before normalization\")\n",
    "# ax[0].set_title(\"(a) Level density\")\n",
    "ax[0].set_ylim(ax[0].get_ylim()[0], ax[0].get_ylim()[1]*2)\n",
    "ax[0].set_ylabel(r\"Level density $\\rho$ [arb. units]\")\n",
    "\n",
    "gsf.to_MeV()\n",
    "gsf.plot(ax=ax[1], scale=\"log\", label=\"result before normalization\")\n",
    "# ax[1].set_title(r\"(b) $\\gamma$SF\")\n",
    "ax[1].set_ylabel(r\"$\\gamma$SF $f$ [arb. units]\")\n",
    "\n",
    "# plot initial guess\n",
    "x0 = extractor.guess_initial_values(E_nld=extractor.nld[0].E, matrix=product, method=\"BSFG-like\")\n",
    "T0 = x0[:product.Eg.size]\n",
    "nld0 = x0[product.Eg.size:]\n",
    "nld0 = om.Vector(values=nld0, E=extractor.nld[0].E/1000)\n",
    "nld0.transform(const=10, inplace=False).plot(ax=ax[0], linestyle=\"--\", markersize=0, alpha=0.7,\n",
    "                                             label=\"BSFG-like initial guess\")\n",
    "\n",
    "x0 = extractor.guess_initial_values(E_nld=extractor.nld[0].E, matrix=product, method=\"CT-like\")\n",
    "T0 = x0[:product.Eg.size]\n",
    "nld0 = x0[product.Eg.size:]\n",
    "nld0 = om.Vector(values=nld0, E=extractor.nld[0].E/1000)\n",
    "nld0.transform(const=5, alpha=0.2, inplace=False).plot(ax=ax[0], linestyle=\"--\", markersize=0, alpha=0.7,\n",
    "                                                       label=\"CT-like initial guess\")\n",
    "\n",
    "gsf0 = om.Vector(values=T0, E=extractor_org.gsf[0].E/1000)\n",
    "gsf0.values /= gsf0.E**3\n",
    "gsf0.transform(const=10e-10, inplace=False).plot(ax=ax[1], linestyle=\"--\", markersize=0, alpha=0.7, \n",
    "                                              label=\"initial guess\")    \n",
    "\n",
    "for ax_ in ax:\n",
    "    ax_.set_yscale(\"log\")\n",
    "    ax_.yaxis.set_major_locator(ticker.LogLocator(numticks=2))\n",
    "\n",
    "ax[0].legend(loc=\"upper left\")\n",
    "ax[1].legend(loc=\"upper right\")\n",
    "    \n",
    "ax[0].set_xlabel(r\"Excitation energy $E_x$ [MeV]\")\n",
    "ax[1].set_xlabel(r\"$\\gamma$-ray energy $E_\\gamma$ [MeV]\")\n",
    "\n",
    "ax[0].text(0.05, 0.05, r\"(a)\",\n",
    "           fontsize=plt.rcParams[\"axes.labelsize\"], \n",
    "           transform = ax[0].transAxes)\n",
    "\n",
    "ax[1].text(0.05, 0.05, r\"(b)\",\n",
    "           fontsize=plt.rcParams[\"axes.labelsize\"], \n",
    "           transform = ax[1].transAxes)   \n",
    "    \n",
    "fig.savefig(f\"figs/nld_gsf_raw_{file_prefix}.pdf\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Does it still look *strange*? probably because you are only used to see the normalized results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Manual normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def plot_transformed(alpha, A=1, B=1):\n",
    "    fig, ax = plt.subplots(1, 2, constrained_layout=True)\n",
    "    for nld, gsf in zip(extractor.nld, extractor.gsf):\n",
    "        nld.transform(const=A, alpha=alpha, inplace=False).plot(ax=ax[0], scale=\"log\", color='k', alpha=1/10)\n",
    "        gsf.transform(const=B, alpha=alpha, inplace=False).plot(ax=ax[1], scale=\"log\", color='k', alpha=1/10)\n",
    "    ax[0].set_title(\"Level density\")\n",
    "    ax[1].set_title(r\"$\\gamma$SF\")\n",
    "\n",
    "plot_transformed(alpha=0.0015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Normalization through external data for one (nld, gsf) set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalization ensures that we find the *physical* solution, so we remove the degeneracy that is in principal inherent to decomposition of NLD and $\\gamma$SF:\n",
    "\\begin{align}\n",
    "NLD' = NLD(E_x) * A exp(\\alpha E_x) \\\\\n",
    "\\gamma SF' = \\gamma SF(E_\\gamma) * B exp(\\alpha E_\\gamma)\n",
    "\\end{align}\n",
    "Note: This is the transformation eq (3), Schiller2000.\n",
    "\n",
    "As external data for the normalization we commonly use:\n",
    "1. the discrete leves, binned with the resolution of our data (and potentially also smoothed)\n",
    "2. The NLD at Sn, derived from D0 and a spin distribution\n",
    "3. The average total radiative width $\\Gamma_\\gamma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Sequentially:\n",
    "Traditionally we have choosen a sequential normalization, where the NLD is normalized first to receive a set $\\alpha$. Then we obtain the scaling parameter $B$ of the $\\gamma$SF from a normalization to the experimental $\\Gamma_\\gamma$.\n",
    "\n",
    "We go though this herem becaue it can be instructive to see. Also, we need to set the parameters, anyhow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### nld normalization  \n",
    "Let's first normalize the mean nld from the extractor.\n",
    "\n",
    "The normalization will take some time (â² 30 seconds). The essential output of multinest is saved to disk, and some output is redirected to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normlog = om.introspection.get_logger('normalizer_nld', 'INFO')\n",
    "normpath = f'normalizer_{file_prefix}'\n",
    "nldnorm = om.NormalizerNLD(nld=nld_mean, discrete=fndiscrete, path=path, \n",
    "                           regenerate=regenerate_normalizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "norm_pars = om.NormalizationParameters(name=\"164Dy\")\n",
    "norm_pars.D0 = D0\n",
    "norm_pars.Sn = [7.658, 0.001] # MeV\n",
    "\n",
    "norm_pars.spincutModel = 'Disc_and_EB05'  # see eg. Guttormsen et al., 2017, PRC 96, 024313\n",
    "# Note: The parameters below gave been adjusted to match the values in Renstrom et al.; \n",
    "# equvivalent of using their reduction factor \\eta = np.sqrt(0.9)\n",
    "norm_pars.spincutPars = {\"mass\":164, \"NLDa\":22.2,\n",
    "                         \"Eshift\":0.31,\n",
    "                         \"Sn\": norm_pars.Sn[0], \"sigma2_disc\":[1.5, 3.6]}\n",
    "    \n",
    "norm_pars.Jtarget = 5/2 # A-1 nucleus\n",
    "\n",
    "nldnorm.bounds[\"A\"] = [0.001, 100]\n",
    "nldnorm.bounds[\"alpha\"] = [1, 2]\n",
    "\n",
    "nldnorm.limit_low = nld_limit_low\n",
    "# nldnorm.limit_high = [3, 6.5]\n",
    "# nldnorm2.normalize(limit_high = [2.2, 4.6])\n",
    "# nldnorm.limit_high = [3, 4.6]\n",
    "nldnorm.limit_high = [3, 6.5]\n",
    "\n",
    "nldnorm.norm_pars = norm_pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nldnorm.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for comparison with the Restrom article\n",
    "nldSn_from_D0 = nldnorm.nldSn_from_D0(**norm_pars.asdict())\n",
    "print(f\"nld(Sn) from D0: {nldSn_from_D0[1]:.2e} MeV^-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nldnorm.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that you might get *strange* results, i.e. unexpected results here, as you use the (potentially erroneous determinated) uncertainties of `nld_mean` in the normalzation, instead of the proper normalization below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\gamma$-SF Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "normlog = om.introspection.get_logger('normalizer_gsf', 'INFO')\n",
    "gsfnorm = om.NormalizerGSF(normalizer_nld=nldnorm, gsf=extractor.gsf[0],\n",
    "                           path=normpath, regenerate=regenerate_normalizers)\n",
    "\n",
    "# to be use for gsf normalization\n",
    "norm_pars.Gg = [113., 13.]  #meV\n",
    "\n",
    "gsfnorm.norm_pars = norm_pars\n",
    "gsfnorm.model_low.Efit=[1.9, 2.5]\n",
    "gsfnorm.model_high.Efit = gsf_model_high_Efit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsfnorm.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gsfnorm.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's often instructive to plot the extrapolation of the $\\gamma$SF; with the interactive code below, we can check the influence of choosing different fit regions. The latest choice is kept for the simultaneous normalization below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gsfnorm.plot_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Simultaneous:\n",
    "We now propose to normalize the NLD and $\\gamma$SF simultaneously instead. This way, we are guaranteed to get matching combinations of the normalization parameters $A$, $B$ and $\\alpha$ for a given ensemble member."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normlog = om.introspection.get_logger('normalizer_simultan', 'INFO')\n",
    "simnorm = om.NormalizerSimultan(normalizer_nld=nldnorm, normalizer_gsf=gsfnorm,\n",
    "                                path=normpath, regenerate=regenerate_normalizers)\n",
    "simnorm.multinest_kwargs[\"n_live_points\"] = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simnorm.normalize(gsf=extractor.gsf[0], nld=extractor.nld[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "simnorm.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Normalization the whole ensemble\n",
    "We have now also developed the toolset to normalize each member of the extractor ensemble separatly. This should provide a statistically more sound and robust normalization. \n",
    "\n",
    "Why so? As the $\\chi^2$ error function is degenerate, a good minimizer should return sets of (nld, gsf) that need to be normalized with different coefficients $A$, $B$ and $\\alpha$. Thus we should normalize each of these sets independently, and build up an uncertainty band only after the normalization. (Instead of the *traditional* approach of normalizing the mean of the sets)\n",
    "\n",
    "Again, you decide whether you normalize sequencially, or, as we **recommend**, to normalize simultaneously.\n",
    "\n",
    "Note that this will this may take several minutes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# normlog = om.introspection.get_logger('ensembleNormalizer', 'INFO')\n",
    "# ensemblenorm_seq = om.EnsembleNormalizer(extractor=extractor, normalizer_nld=nldnorm,\n",
    "#                                      normalizer_gsf=gsfnorm)\n",
    "# ensemblenorm_seq.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemblenorm_seq.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simultaneous normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ensemblenorm_sim = om.EnsembleNormalizer(extractor=extractor, normalizer_simultan=simnorm\n",
    "                                         path=normpath, regenerate=regenerate_normalizers))\n",
    "ensemblenorm_sim.normalizer_simultan.multinest_kwargs[\"n_live_points\"] = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemblenorm_sim.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ensemblenorm_sim.plot(n_plot=60);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare to oslo-method-software\n",
    "For reference to the dataset, see [here](#loading_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if case.index in [0, 1]:\n",
    "    fig, ax = plt.subplots(1, 2, constrained_layout=True)\n",
    "    _, _, stats = ensemblenorm_sim.plot(ax, n_plot=1, add_figlegend=False,\n",
    "                         plot_model_stats=True,\n",
    "                         random_state=np.random.default_rng(6577),\n",
    "                                       return_stats=True)\n",
    "\n",
    "\n",
    "    colors = plt.rcParams['axes.prop_cycle'].by_key()[\"color\"]\n",
    "    color = colors[7]\n",
    "\n",
    "    # Load and add published, normalized data:\n",
    "    # format:\n",
    "    # Eg(MeV)\t nld(MeV^-1)\t err_rsg(MeV^-1)\t err_tot(MeV^-1)\n",
    "    label=r\"Renstr{\\o}m $\\textit{et\\,\\,al.}$\"\n",
    "    nld_published = np.loadtxt(\"misc_data/nld_164Dy_3He_3He_164Dy.txt\", skiprows=6)\n",
    "    def plt_err(ax, arr, yerr=\"total\", fmt=\"^\", color=color,\n",
    "                markerfacecolor='none', **kwargs):\n",
    "        if yerr == \"total\":\n",
    "            yerr = arr[:, 3]\n",
    "        elif yerr == \"statistical\":\n",
    "            yerr = arr[:, 2]\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        ax.errorbar(arr[:, 0], arr[:, 1], yerr=yerr,\n",
    "                       fmt=fmt, color=color, markerfacecolor=markerfacecolor,\n",
    "                       **kwargs)\n",
    "\n",
    "    plt_err(ax[0], nld_published, yerr=\"statistical\", label=label)\n",
    "    plt_err(ax[0], nld_published, yerr=\"total\", label=None, capsize=0, alpha=0.5)\n",
    "\n",
    "    # Load and add published, normalized data:\n",
    "    # format:\n",
    "    # Eg(MeV)\t f(MeV^-3)\t err_rsg(MeV^-3)\t err_tot(MeV^-3)\n",
    "    gsf_published = np.loadtxt(\"misc_data/gsf_164Dy_3He_3He_164Dy.txt\")\n",
    "    plt_err(ax[1], gsf_published, yerr=\"statistical\")\n",
    "    plt_err(ax[1], gsf_published, yerr=\"total\", capsize=0, alpha=0.5)\n",
    "\n",
    "    for axis in ax:\n",
    "        for l in axis.lines:\n",
    "            if l._label in [\"exp.\", \"_exp.\"]:\n",
    "                l._markersize=3\n",
    "            if l._label in [\"exp.\"]: # rename\n",
    "                l._label=\"sample\"\n",
    "            if l._label in [\"median\"]: # rename\n",
    "                l._label=\"results\"\n",
    "\n",
    "    # shuffel labels\n",
    "    handles, labels = ax[0].get_legend_handles_labels()\n",
    "    \n",
    "    new_indexes = [3, 6, 0, 4, 7, 5, 1, 8, 9]\n",
    "    handles, labels = np.array(handles), np.array(labels)\n",
    "    handles, labels = handles[new_indexes], labels[new_indexes]\n",
    "    handles[0] = (handles[1], handles[0]),\n",
    "    handles[1] = None\n",
    "    handles[3] = (handles[4], handles[3]),\n",
    "    handles[4] = None\n",
    "    \n",
    "    ax[0].legend(handles, labels, loc=\"best\")\n",
    "\n",
    "    ax[0].text(0.9, 0.05, r\"(a)\",\n",
    "               fontsize=plt.rcParams[\"axes.labelsize\"], \n",
    "               transform = ax[0].transAxes)\n",
    "\n",
    "    ax[1].text(0.9, 0.05, r\"(b)\",\n",
    "               fontsize=plt.rcParams[\"axes.labelsize\"], \n",
    "               transform = ax[1].transAxes)  \n",
    "\n",
    "    fig.set_size_inches((8, 4))\n",
    "    # fig.tight_layout()\n",
    "    fig.savefig(f\"figs/normalized_{file_prefix}.pdf\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try comparison in one figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    file_prefix_ = \"164Dy\"\n",
    "    fname_ensemblenorm_sim = f'ensemblenorm_sim_{file_prefix_}.pkl'\n",
    "    with open(fname_ensemblenorm_sim, 'rb') as fobj:\n",
    "        ensemblenorm_sim_org = dill.load(fobj)\n",
    "\n",
    "    file_prefix_ = \"164Dy_reduced\"\n",
    "    fname_ensemblenorm_sim = f'ensemblenorm_sim_{file_prefix_}.pkl'\n",
    "    with open(fname_ensemblenorm_sim, 'rb') as fobj:\n",
    "        ensemblenorm_sim_reduced = dill.load(fobj)\n",
    "except FileNotFoundError:\n",
    "    print(\"could not find files\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, constrained_layout=True)\n",
    "_, _, stats_org = ensemblenorm_sim_org.plot(ax, n_plot=1, add_figlegend=False,\n",
    "                     plot_model_stats=True,\n",
    "                     random_state=np.random.default_rng(12345669),\n",
    "                                   return_stats=True)\n",
    "\n",
    "# plot reduced nld_ on diff. scale\n",
    "ax0_twin = ax[0].twinx()\n",
    "ax1_twin = ax[1].twinx()\n",
    "_, _, stats_sim = ensemblenorm_sim_reduced.plot([ax0_twin, ax1_twin], n_plot=1, add_figlegend=False,\n",
    "                     plot_model_stats=True,\n",
    "                     random_state=np.random.default_rng(12345),\n",
    "                                   return_stats=True)\n",
    "\n",
    "ax[1].set_ylim(2e-9, None)\n",
    "scaler = 5\n",
    "ax0_twin.set_ylim(np.array(ax[0].get_ylim())*scaler)\n",
    "ax1_twin.set_ylim(np.array(ax[1].get_ylim())*scaler)\n",
    "\n",
    "# remove additional labels\n",
    "for tax in [ax0_twin, ax1_twin]:\n",
    "    tax.tick_params(axis='y', which='both', right=False, labelright=False)\n",
    "    tax.set_ylabel(None)\n",
    "    tax.patches = []  # remove (additional) \"fit region\" patch\n",
    "    \n",
    "for axis in ax:\n",
    "    axis.set_xlim(0, None)\n",
    "    for l in axis.lines:\n",
    "        if l._label in [\"exp.\", \"_exp.\"]:\n",
    "            l._markersize=3\n",
    "        if l._label in [\"exp.\"]: # rename\n",
    "            l._label=\"sample\"\n",
    "        if l._label in [\"median\"]: # rename\n",
    "            l._label=\"results\"\n",
    "            \n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "\n",
    "new_indexes = [3, 6, 0, 4, 7, 5, 1, 8]\n",
    "handles, labels = np.array(handles), np.array(labels)\n",
    "handles, labels = handles[new_indexes], labels[new_indexes]\n",
    "handles[0] = (handles[1], handles[0]),\n",
    "handles[1] = None\n",
    "handles[3] = (handles[4], handles[3]),\n",
    "handles[4] = None\n",
    "\n",
    "ax[0].legend(handles, labels, loc=\"best\")\n",
    "            \n",
    "ax[0].text(0.7, 0.5, f\"x 1/{scaler}\",\n",
    "           fontsize=plt.rcParams[\"axes.labelsize\"], \n",
    "           transform = ax[0].transAxes)\n",
    "ax[1].text(0.7, 0.35, f\"x 1/{scaler}\",\n",
    "           fontsize=plt.rcParams[\"axes.labelsize\"], \n",
    "           transform = ax[1].transAxes)\n",
    "\n",
    "ax[0].text(0.9, 0.05, r\"(a)\",\n",
    "           fontsize=plt.rcParams[\"axes.labelsize\"], \n",
    "           transform = ax[0].transAxes)\n",
    "\n",
    "ax[1].text(0.9, 0.05, r\"(b)\",\n",
    "           fontsize=plt.rcParams[\"axes.labelsize\"], \n",
    "           transform = ax[1].transAxes)  \n",
    "\n",
    "fig.set_size_inches((8, 4))\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"figs/normalized_comparison.pdf\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic data vs RAINIER input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_compare_RAINIER(normalizer, \n",
    "                         fname=\"compare_to_RAINIER.pdf\"):\n",
    "    fig, ax = plt.subplots(1, 2, constrained_layout=True)\n",
    "    normalizer.plot(ax, n_plot=1, add_figlegend=False,\n",
    "                          plot_model_stats=True, \n",
    "                          random_state=np.random.default_rng(123456))\n",
    "\n",
    "    # load 'input' NLD and gsf\n",
    "    # note that for the nld we should take the output from RAINIER (actual created level scheme)\n",
    "    # here we load the continuum bins\n",
    "    # format: Energy, levels_created [this bin], levels_model [MeV^-1], levels_model_from_tot [MeV^-1]\n",
    "    nld_input = np.loadtxt(\"RAINIER_164Dy/NLDcont.dat\")\n",
    "\n",
    "    # # format: E, fE1+fM1, fE1, fM1\n",
    "    gsf_input = np.loadtxt(\"RAINIER_164Dy/gsf.txt\")\n",
    "    # Gg from RAINIER in 153 meV -> scale gsf to get same Gg as here\n",
    "    gsf_input[:, 1] *= norm_pars.Gg[0] / 153\n",
    "\n",
    "\n",
    "    ax[0].plot(nld_input[:,0], nld_input[:,2], \"r\", label=\"input\")\n",
    "    ax[1].plot(gsf_input[:,0], gsf_input[:,1], \"r\")\n",
    "    \n",
    "    \n",
    "    # remove additional labels\n",
    "    #for tax in [ax0_twin, ax1_twin]:\n",
    "    #    tax.tick_params(axis='y', which='both', right=False, labelright=False)\n",
    "    #    tax.set_ylabel(None)\n",
    "    #    tax.patches = []  # remove (additional) \"fit region\" patch\n",
    "\n",
    "    for axis in ax:\n",
    "        axis.set_xlim(0, None)\n",
    "        for l in axis.lines:\n",
    "            if l._label in [\"exp.\", \"_exp.\"]:\n",
    "                l._markersize=3\n",
    "            if l._label in [\"exp.\"]: # rename\n",
    "                l._label=\"sample\"\n",
    "            if l._label in [\"median\"]: # rename\n",
    "                l._label=\"results\"\n",
    "\n",
    "    handles, labels = ax[0].get_legend_handles_labels()\n",
    "\n",
    "    new_indexes = [5, 3, 7, 0, 4, 8, 6, 1, 9]\n",
    "    handles, labels = np.array(handles), np.array(labels)\n",
    "    handles, labels = handles[new_indexes], labels[new_indexes]\n",
    "    handles[1] = (handles[2], handles[1]),\n",
    "    handles[2] = None\n",
    "    handles[4] = (handles[5], handles[4]),\n",
    "    handles[5] = None\n",
    "    \n",
    "    ax[0].legend(handles, labels, loc=\"best\")\n",
    "\n",
    "    fig.set_size_inches((8, 4))\n",
    "    fig.savefig(\"figs/\"+fname)\n",
    "if case.index==2:\n",
    "    plot_compare_RAINIER(ensemblenorm_sim, fname=\"compare_to_RAINIER.pdf\")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
